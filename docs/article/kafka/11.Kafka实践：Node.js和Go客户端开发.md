---
title: Kafka实践：Node.js和Go客户端开发
date: 2026-01-15
description: 学习如何在Node.js和Go中使用Kafka客户端库进行消息生产和消费
---

# Kafka实践：Node.js和Go客户端开发

本教程将详细介绍如何在Node.js和Go语言中使用Kafka客户端库进行消息的生产和消费。

## 一、Node.js使用Kafka

### 1.1 安装依赖

Node.js中最流行的Kafka客户端库是`kafkajs`。

```bash
# 使用npm安装
npm install kafkajs

# 或使用yarn
yarn add kafkajs

# 或使用pnpm
pnpm add kafkajs
```

### 1.2 创建Kafka客户端

```javascript
const { Kafka } = require('kafkajs');

// 创建Kafka客户端实例
const kafka = new Kafka({
  clientId: 'my-app',
  brokers: ['localhost:9092'], // Kafka broker地址
  // 可选配置
  retry: {
    initialRetryTime: 100,
    retries: 8
  }
});
```

### 1.3 Producer（生产者）示例

#### 基础Producer

```javascript
const { Kafka } = require('kafkajs');

const kafka = new Kafka({
  clientId: 'my-producer',
  brokers: ['localhost:9092']
});

const producer = kafka.producer();

async function sendMessage() {
  await producer.connect();
  
  try {
    // 发送单条消息
    await producer.send({
      topic: 'test-topic',
      messages: [
        { value: 'Hello Kafka from Node.js!' }
      ]
    });
    
    console.log('消息发送成功');
  } catch (error) {
    console.error('发送消息失败:', error);
  } finally {
    await producer.disconnect();
  }
}

sendMessage();
```

#### 发送带Key的消息

```javascript
async function sendMessageWithKey() {
  await producer.connect();
  
  try {
    await producer.send({
      topic: 'test-topic',
      messages: [
        {
          key: 'user-123', // 相同key会发送到同一分区
          value: JSON.stringify({
            userId: '123',
            action: 'login',
            timestamp: Date.now()
          })
        }
      ]
    });
    
    console.log('消息发送成功');
  } catch (error) {
    console.error('发送消息失败:', error);
  } finally {
    await producer.disconnect();
  }
}
```

#### 批量发送消息

```javascript
async function sendBatchMessages() {
  await producer.connect();
  
  try {
    const messages = [];
    for (let i = 0; i < 100; i++) {
      messages.push({
        value: `Message ${i}`
      });
    }
    
    await producer.send({
      topic: 'test-topic',
      messages: messages
    });
    
    console.log('批量消息发送成功');
  } catch (error) {
    console.error('发送消息失败:', error);
  } finally {
    await producer.disconnect();
  }
}
```

#### 异步发送（推荐）

```javascript
async function sendAsync() {
  await producer.connect();
  
  // 异步发送，不等待响应
  producer.send({
    topic: 'test-topic',
    messages: [{ value: 'Async message' }]
  }).then(result => {
    console.log('消息发送成功:', result);
  }).catch(error => {
    console.error('发送失败:', error);
  });
  
  // 继续执行其他操作
  console.log('继续执行其他操作...');
}
```

### 1.4 Consumer（消费者）示例

#### 基础Consumer

```javascript
const { Kafka } = require('kafkajs');

const kafka = new Kafka({
  clientId: 'my-consumer',
  brokers: ['localhost:9092']
});

const consumer = kafka.consumer({ 
  groupId: 'my-group' // Consumer Group ID
});

async function consumeMessages() {
  await consumer.connect();
  
  // 订阅Topic
  await consumer.subscribe({ 
    topic: 'test-topic',
    fromBeginning: true // 从开始位置消费
  });
  
  // 消费消息
  await consumer.run({
    eachMessage: async ({ topic, partition, message }) => {
      console.log({
        topic,
        partition,
        offset: message.offset,
        value: message.value.toString()
      });
      
      // 处理消息
      try {
        const data = JSON.parse(message.value.toString());
        console.log('处理消息:', data);
      } catch (error) {
        console.log('原始消息:', message.value.toString());
      }
    }
  });
}

// 优雅关闭
process.on('SIGINT', async () => {
  console.log('正在关闭Consumer...');
  await consumer.disconnect();
  process.exit(0);
});

consumeMessages().catch(console.error);
```

#### 手动提交Offset

```javascript
await consumer.run({
  eachMessage: async ({ topic, partition, message }) => {
    try {
      // 处理消息
      console.log('处理消息:', message.value.toString());
      
      // 手动提交Offset
      await consumer.commitOffsets([{
        topic,
        partition,
        offset: (parseInt(message.offset) + 1).toString()
      }]);
    } catch (error) {
      console.error('处理消息失败:', error);
      // 可以选择不提交Offset，消息会被重新消费
    }
  }
});
```

#### 消费多个Topic

```javascript
await consumer.subscribe({ 
  topics: ['topic-1', 'topic-2', 'topic-3'],
  fromBeginning: false
});
```

### 1.5 完整示例：Producer和Consumer

#### Producer示例

```javascript
// producer.js
const { Kafka } = require('kafkajs');

const kafka = new Kafka({
  clientId: 'my-producer',
  brokers: ['localhost:9092']
});

const producer = kafka.producer();

async function main() {
  await producer.connect();
  console.log('Producer已连接');
  
  // 每秒发送一条消息
  setInterval(async () => {
    try {
      await producer.send({
        topic: 'test-topic',
        messages: [{
          key: `key-${Date.now()}`,
          value: JSON.stringify({
            timestamp: Date.now(),
            message: `Message at ${new Date().toISOString()}`
          })
        }]
      });
      console.log('消息已发送');
    } catch (error) {
      console.error('发送失败:', error);
    }
  }, 1000);
}

main().catch(console.error);
```

#### Consumer示例

```javascript
// consumer.js
const { Kafka } = require('kafkajs');

const kafka = new Kafka({
  clientId: 'my-consumer',
  brokers: ['localhost:9092']
});

const consumer = kafka.consumer({ 
  groupId: 'my-group'
});

async function main() {
  await consumer.connect();
  console.log('Consumer已连接');
  
  await consumer.subscribe({ 
    topic: 'test-topic',
    fromBeginning: false
  });
  
  await consumer.run({
    eachMessage: async ({ topic, partition, message }) => {
      console.log({
        topic,
        partition,
        offset: message.offset,
        key: message.key?.toString(),
        value: message.value.toString()
      });
    }
  });
}

// 优雅关闭
process.on('SIGINT', async () => {
  console.log('正在关闭...');
  await consumer.disconnect();
  process.exit(0);
});

main().catch(console.error);
```

### 1.6 错误处理和重试

```javascript
const producer = kafka.producer({
  retry: {
    initialRetryTime: 100,
    retries: 8,
    multiplier: 2,
    maxRetryTime: 30000
  }
});

// 监听错误事件
producer.on('producer.connect', () => {
  console.log('Producer已连接');
});

producer.on('producer.disconnect', () => {
  console.log('Producer已断开');
});

producer.on('producer.network.request_timeout', (payload) => {
  console.error('请求超时:', payload);
});
```

## 二、Go使用Kafka

### 2.1 安装依赖

Go语言中最流行的Kafka客户端库是`confluent-kafka-go`（基于librdkafka）和`segmentio/kafka-go`（纯Go实现）。

#### 方式一：使用segmentio/kafka-go（推荐，纯Go）

```bash
go get github.com/segmentio/kafka-go
```

#### 方式二：使用confluent-kafka-go（需要C库）

```bash
# 需要先安装librdkafka
# macOS
brew install librdkafka

# Ubuntu/Debian
sudo apt-get install librdkafka-dev

# 然后安装Go库
go get github.com/confluentinc/confluent-kafka-go/kafka
```

### 2.2 使用segmentio/kafka-go

#### Producer示例

```go
package main

import (
    "context"
    "fmt"
    "log"
    "time"

    "github.com/segmentio/kafka-go"
)

func main() {
    // 创建Writer
    conn, err := kafka.DialLeader(context.Background(), "tcp", "localhost:9092", "test-topic", 0)
    if err != nil {
        log.Fatal("连接失败:", err)
    }
    defer conn.Close()

    // 设置写入超时
    conn.SetWriteDeadline(time.Now().Add(10 * time.Second))

    // 发送消息
    _, err = conn.WriteMessages(
        kafka.Message{Value: []byte("Hello Kafka from Go!")},
    )
    if err != nil {
        log.Fatal("发送失败:", err)
    }

    fmt.Println("消息发送成功")
}
```

#### 使用Writer（推荐方式）

```go
package main

import (
    "context"
    "fmt"
    "log"
    "time"

    "github.com/segmentio/kafka-go"
)

func main() {
    // 创建Writer
    w := &kafka.Writer{
        Addr:     kafka.TCP("localhost:9092"),
        Topic:    "test-topic",
        Balancer: &kafka.LeastBytes{}, // 负载均衡策略
    }
    defer w.Close()

    // 发送消息
    err := w.WriteMessages(context.Background(),
        kafka.Message{
            Key:   []byte("key-1"),
            Value: []byte("Hello Kafka!"),
        },
        kafka.Message{
            Key:   []byte("key-2"),
            Value: []byte("Another message"),
        },
    )
    if err != nil {
        log.Fatal("发送失败:", err)
    }

    fmt.Println("消息发送成功")
}
```

#### 异步发送

```go
func sendAsync() {
    w := &kafka.Writer{
        Addr:         kafka.TCP("localhost:9092"),
        Topic:        "test-topic",
        Async:        true, // 异步模式
        BatchSize:    100,  // 批量大小
        BatchTimeout: 10 * time.Millisecond,
    }
    defer w.Close()

    // 异步发送，立即返回
    err := w.WriteMessages(context.Background(),
        kafka.Message{Value: []byte("Async message")},
    )
    if err != nil {
        log.Fatal("发送失败:", err)
    }

    fmt.Println("消息已提交发送")
}
```

#### Consumer示例

```go
package main

import (
    "context"
    "fmt"
    "log"

    "github.com/segmentio/kafka-go"
)

func main() {
    // 创建Reader
    r := kafka.NewReader(kafka.ReaderConfig{
        Brokers:  []string{"localhost:9092"},
        Topic:    "test-topic",
        GroupID:  "my-group", // Consumer Group ID
        MinBytes: 10e3,       // 10KB
        MaxBytes: 10e6,       // 10MB
    })
    defer r.Close()

    fmt.Println("开始消费消息...")

    for {
        // 读取消息
        m, err := r.ReadMessage(context.Background())
        if err != nil {
            log.Fatal("读取失败:", err)
        }

        fmt.Printf("消息: topic=%s partition=%d offset=%d key=%s value=%s\n",
            m.Topic, m.Partition, m.Offset, string(m.Key), string(m.Value))
    }
}
```

#### 批量消费

```go
func consumeBatch() {
    r := kafka.NewReader(kafka.ReaderConfig{
        Brokers:  []string{"localhost:9092"},
        Topic:    "test-topic",
        GroupID:  "my-group",
        MinBytes: 10e3,
        MaxBytes: 10e6,
        MaxWait:  1 * time.Second, // 等待时间
    })
    defer r.Close()

    for {
        // 批量读取
        batch := r.ReadBatch(context.Background(), 10e3, 10e6)
        
        for {
            m, err := batch.ReadMessage()
            if err != nil {
                break
            }
            fmt.Printf("消息: %s\n", string(m.Value))
        }
        
        batch.Close()
    }
}
```

### 2.3 使用confluent-kafka-go

#### Producer示例

```go
package main

import (
    "fmt"
    "log"

    "github.com/confluentinc/confluent-kafka-go/kafka"
)

func main() {
    // 创建Producer
    p, err := kafka.NewProducer(&kafka.ConfigMap{
        "bootstrap.servers": "localhost:9092",
        "client.id":         "my-producer",
    })
    if err != nil {
        log.Fatal("创建Producer失败:", err)
    }
    defer p.Close()

    // 发送消息
    topic := "test-topic"
    message := "Hello Kafka from Go (confluent)!"

    err = p.Produce(&kafka.Message{
        TopicPartition: kafka.TopicPartition{
            Topic:     &topic,
            Partition: kafka.PartitionAny,
        },
        Value: []byte(message),
    }, nil)

    if err != nil {
        log.Fatal("发送失败:", err)
    }

    // 等待消息发送完成
    p.Flush(15 * 1000)
    fmt.Println("消息发送成功")
}
```

#### Consumer示例

```go
package main

import (
    "fmt"
    "log"

    "github.com/confluentinc/confluent-kafka-go/kafka"
)

func main() {
    // 创建Consumer
    c, err := kafka.NewConsumer(&kafka.ConfigMap{
        "bootstrap.servers": "localhost:9092",
        "group.id":          "my-group",
        "auto.offset.reset": "earliest",
    })
    if err != nil {
        log.Fatal("创建Consumer失败:", err)
    }
    defer c.Close()

    // 订阅Topic
    err = c.SubscribeTopics([]string{"test-topic"}, nil)
    if err != nil {
        log.Fatal("订阅失败:", err)
    }

    fmt.Println("开始消费消息...")

    for {
        // 读取消息
        msg, err := c.ReadMessage(-1)
        if err != nil {
            log.Printf("读取失败: %v\n", err)
            continue
        }

        fmt.Printf("消息: topic=%s partition=%d offset=%d value=%s\n",
            *msg.TopicPartition.Topic, msg.TopicPartition.Partition,
            msg.TopicPartition.Offset, string(msg.Value))
    }
}
```

## 三、完整项目示例

### 3.1 Node.js完整项目

#### 项目结构

```
kafka-nodejs/
├── package.json
├── producer.js
├── consumer.js
└── config.js
```

#### config.js

```javascript
module.exports = {
  kafka: {
    brokers: ['localhost:9092'],
    clientId: 'my-app'
  },
  topics: {
    userEvents: 'user-events',
    orders: 'orders'
  }
};
```

#### producer.js

```javascript
const { Kafka } = require('kafkajs');
const config = require('./config');

const kafka = new Kafka(config.kafka);
const producer = kafka.producer();

async function startProducer() {
  await producer.connect();
  console.log('Producer已启动');

  // 模拟发送用户事件
  setInterval(async () => {
    const event = {
      userId: Math.floor(Math.random() * 1000),
      action: ['login', 'logout', 'purchase'][Math.floor(Math.random() * 3)],
      timestamp: Date.now()
    };

    try {
      await producer.send({
        topic: config.topics.userEvents,
        messages: [{
          key: `user-${event.userId}`,
          value: JSON.stringify(event)
        }]
      });
      console.log('事件已发送:', event);
    } catch (error) {
      console.error('发送失败:', error);
    }
  }, 2000);
}

startProducer().catch(console.error);
```

#### consumer.js

```javascript
const { Kafka } = require('kafkajs');
const config = require('./config');

const kafka = new Kafka(config.kafka);
const consumer = kafka.consumer({ groupId: 'event-processor' });

async function startConsumer() {
  await consumer.connect();
  console.log('Consumer已连接');

  await consumer.subscribe({
    topics: [config.topics.userEvents],
    fromBeginning: false
  });

  await consumer.run({
    eachMessage: async ({ topic, partition, message }) => {
      try {
        const event = JSON.parse(message.value.toString());
        console.log('处理事件:', event);

        // 根据事件类型处理
        switch (event.action) {
          case 'login':
            console.log(`用户 ${event.userId} 登录`);
            break;
          case 'logout':
            console.log(`用户 ${event.userId} 登出`);
            break;
          case 'purchase':
            console.log(`用户 ${event.userId} 购买商品`);
            break;
        }
      } catch (error) {
        console.error('处理消息失败:', error);
      }
    }
  });
}

process.on('SIGINT', async () => {
  console.log('正在关闭Consumer...');
  await consumer.disconnect();
  process.exit(0);
});

startConsumer().catch(console.error);
```

### 3.2 Go完整项目

#### 项目结构

```
kafka-go/
├── go.mod
├── go.sum
├── cmd/
│   ├── producer/
│   │   └── main.go
│   └── consumer/
│       └── main.go
└── pkg/
    └── kafka/
        └── client.go
```

#### pkg/kafka/client.go

```go
package kafka

import (
    "context"
    "github.com/segmentio/kafka-go"
)

type Client struct {
    brokers []string
}

func NewClient(brokers []string) *Client {
    return &Client{brokers: brokers}
}

func (c *Client) NewProducer(topic string) *kafka.Writer {
    return &kafka.Writer{
        Addr:     kafka.TCP(c.brokers...),
        Topic:    topic,
        Balancer: &kafka.LeastBytes{},
    }
}

func (c *Client) NewConsumer(topic, groupID string) *kafka.Reader {
    return kafka.NewReader(kafka.ReaderConfig{
        Brokers:  c.brokers,
        Topic:    topic,
        GroupID:  groupID,
        MinBytes: 10e3,
        MaxBytes: 10e6,
    })
}
```

#### cmd/producer/main.go

```go
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "log"
    "time"

    "github.com/segmentio/kafka-go"
)

type UserEvent struct {
    UserID    int    `json:"userId"`
    Action    string `json:"action"`
    Timestamp int64  `json:"timestamp"`
}

func main() {
    w := &kafka.Writer{
        Addr:     kafka.TCP("localhost:9092"),
        Topic:    "user-events",
        Balancer: &kafka.LeastBytes{},
    }
    defer w.Close()

    actions := []string{"login", "logout", "purchase"}

    for i := 0; i < 10; i++ {
        event := UserEvent{
            UserID:    i % 100,
            Action:    actions[i%3],
            Timestamp: time.Now().Unix(),
        }

        data, _ := json.Marshal(event)

        err := w.WriteMessages(context.Background(),
            kafka.Message{
                Key:   []byte(fmt.Sprintf("user-%d", event.UserID)),
                Value: data,
            },
        )
        if err != nil {
            log.Fatal("发送失败:", err)
        }

        fmt.Printf("事件已发送: %+v\n", event)
        time.Sleep(1 * time.Second)
    }
}
```

#### cmd/consumer/main.go

```go
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "log"
    "os"
    "os/signal"
    "syscall"

    "github.com/segmentio/kafka-go"
)

type UserEvent struct {
    UserID    int    `json:"userId"`
    Action    string `json:"action"`
    Timestamp int64  `json:"timestamp"`
}

func main() {
    r := kafka.NewReader(kafka.ReaderConfig{
        Brokers:  []string{"localhost:9092"},
        Topic:    "user-events",
        GroupID:  "event-processor",
        MinBytes: 10e3,
        MaxBytes: 10e6,
    })
    defer r.Close()

    fmt.Println("开始消费消息...")

    // 优雅关闭
    sigChan := make(chan os.Signal, 1)
    signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

    go func() {
        <-sigChan
        fmt.Println("\n正在关闭Consumer...")
        r.Close()
        os.Exit(0)
    }()

    for {
        m, err := r.ReadMessage(context.Background())
        if err != nil {
            log.Printf("读取失败: %v\n", err)
            continue
        }

        var event UserEvent
        if err := json.Unmarshal(m.Value, &event); err != nil {
            log.Printf("解析失败: %v\n", err)
            continue
        }

        fmt.Printf("处理事件: %+v\n", event)

        // 根据事件类型处理
        switch event.Action {
        case "login":
            fmt.Printf("用户 %d 登录\n", event.UserID)
        case "logout":
            fmt.Printf("用户 %d 登出\n", event.UserID)
        case "purchase":
            fmt.Printf("用户 %d 购买商品\n", event.UserID)
        }
    }
}
```

## 四、最佳实践

### 4.1 Node.js最佳实践

1. **连接池管理**：复用Producer和Consumer实例
2. **错误处理**：始终处理异步操作的错误
3. **优雅关闭**：监听SIGINT/SIGTERM信号，正确关闭连接
4. **批量发送**：使用批量发送提高性能
5. **监控和日志**：记录关键操作和错误

### 4.2 Go最佳实践

1. **资源管理**：使用defer确保资源正确释放
2. **Context使用**：使用context控制超时和取消
3. **错误处理**：检查所有可能返回错误的操作
4. **并发安全**：如果多个goroutine使用同一个Writer/Reader，注意并发安全
5. **优雅关闭**：监听系统信号，正确关闭连接

## 五、常见问题

### 5.1 Node.js常见问题

**问题1**：消息发送后Consumer收不到

**解决**：检查Consumer是否使用了正确的Consumer Group，以及是否从正确的位置开始消费。

**问题2**：连接超时

**解决**：检查Kafka broker地址和端口是否正确，网络是否通畅。

### 5.2 Go常见问题

**问题1**：confluent-kafka-go编译失败

**解决**：确保已安装librdkafka库，或使用纯Go实现的segmentio/kafka-go。

**问题2**：消息顺序问题

**解决**：确保使用相同的Key发送到同一Partition，或使用单Partition Topic。

## 六、总结

- **Node.js**：使用`kafkajs`库，API简洁，易于使用
- **Go**：推荐使用`segmentio/kafka-go`（纯Go）或`confluent-kafka-go`（性能更好但需要C库）
- **Producer**：注意批量发送和错误处理
- **Consumer**：注意Offset管理和优雅关闭
- **最佳实践**：连接复用、错误处理、优雅关闭、监控日志

掌握这些客户端库的使用，你就可以在实际项目中集成Kafka了！
